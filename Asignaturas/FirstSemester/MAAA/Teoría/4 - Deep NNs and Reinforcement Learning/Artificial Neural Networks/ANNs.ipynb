{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"title\">Artificial Neural Networks</div>\n",
    "<div class=\"subtitle\">Métodos Avanzados en Aprendizaje Automático</div>\n",
    "<div class=\"author\">Carlos María Alaíz Gudín - Universidad Autónoma de Madrid</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the configuration of Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<head><link rel=\"stylesheet\" href=\"style.css\"></head>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<head><link rel=\"stylesheet\" href=\"style.css\"></head>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the packages to be used (all of them quite standard except for `Utils`, which is provided with the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from Utils import plot_dataset_clas, plot_linear_model_clas, plot_perceptron_evo_iter\n",
    "from Utils import plot_nonlinear_model_clas\n",
    "\n",
    "matplotlib.rc('figure', figsize=(15, 5))\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the AND dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([-1, -1, -1, 1])\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_dataset_clas(x, y_and)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the problem linearly separable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a perceptron over the AND dataset using `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_and = Perceptron(tol=1e-3, random_state=seed)\n",
    "model_and.fit(x, y_and)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_linear_model_clas(x, y_and, model_and.coef_[0], model_and.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the model perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the evolution of the learning algorithm using a custom implementation (hence the resultant model can be different from the one above).\n",
    "\n",
    "The circled pattern is the sample analysed in the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_perceptron_evo_iter(x, y_and, max_iters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* When is the model updated?\n",
    "* Does this match the theory of Rosenblatt Perceptron?\n",
    "* Does the algorithm converge?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR  Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the OR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_or = np.array([-1, 1, 1, 1])\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_dataset_clas(x, y_or)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the problem linearly separable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a perceptron over the OR dataset using `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_or = Perceptron(tol=1e-3, random_state=seed)\n",
    "model_or.fit(x, y_or)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_linear_model_clas(x, y_or, model_or.coef_[0], model_or.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the model perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the evolution of the learning algorithm using a custom implementation (hence the resultant model can be different from the one above).\n",
    "\n",
    "The circled pattern is the sample analysed in the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_perceptron_evo_iter(x, y_or, max_iters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* When is the model updated?\n",
    "* Does this match the theory of Rosenblatt Perceptron?\n",
    "* Does the algorithm converge?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR  Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the XOR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xor = np.array([-1, 1, 1, -1])\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_dataset_clas(x, y_xor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the problem linearly separable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a perceptron over the XOR dataset using `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_xor = Perceptron(tol=1e-3, random_state=seed)\n",
    "model_xor.fit(x, y_xor)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_linear_model_clas(x, y_xor, model_xor.coef_[0], model_xor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the model perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the evolution of the learning algorithm using a custom implementation (hence the resultant model can be different from the one above).\n",
    "\n",
    "The circled pattern is the sample analysed in the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perceptron_evo_iter(x, y_xor, max_iters=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* When is the model updated?\n",
    "* Does this match the theory of Rosenblatt Perceptron?\n",
    "* Does the algorithm converge?\n",
    "* Is the algorithm trapped in a loop?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates a 2-dimensional classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 200\n",
    "noise = 0.15\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x_moon, y_moon = make_moons(n_samples=n_pat, noise=noise, random_state=seed)\n",
    "y_moon[y_moon == 0] = -1\n",
    "plot_dataset_clas(x_moon, y_moon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the problem linearly separable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a perceptron over the previous dataset using `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron(tol=1e-3)\n",
    "model.fit(x_moon, y_moon)\n",
    "\n",
    "plot_linear_model_clas(x_moon, y_moon, model.coef_[0], model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the model perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR gate satisfies: XOR = AND(NOT AND, OR). This equality can be used to build a Neural Network solving the XOR gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_and = model_and.predict(x)\n",
    "print(\"AND:\", pred_and)\n",
    "\n",
    "pred_nand = -model_and.predict(x)\n",
    "print(\"NAND:\", pred_and)\n",
    "\n",
    "pred_or = model_or.predict(x)\n",
    "print(\"OR:\", pred_or)\n",
    "\n",
    "pred_xor = model_and.predict(np.column_stack((pred_nand, pred_or)))\n",
    "print(\"XOR:\", pred_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Check how these results correspond to those of the theory (but with a $-1/1$ encoding).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model can be defined using this stacked architecture as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_predict(x):\n",
    "    return model_and.predict(np.column_stack((- model_and.predict(x), model_or.predict(x))))\n",
    "\n",
    "model_xor.predict = xor_predict\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_nonlinear_model_clas(x, y_xor, model_xor, phi=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is this model linear?\n",
    "* How does it perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below trains an MLP with a single hidden layer, and only two hidden units.\n",
    "According to the theory, this model is complex enough to solve the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(2,), max_iter=1000)\n",
    "model.fit(x, y_xor)\n",
    "\n",
    "print(\"Weights (%dx%d + %d): \" % (model.coefs_[0].shape[0], model.coefs_[0].shape[1], len(model.coefs_[1])), end=\"\")\n",
    "print(model.coefs_)\n",
    "print(\"Intercepts (%d + %d): \" % (len(model.intercepts_[0]), len(model.intercepts_[1])), end=\"\")\n",
    "print(model.intercepts_)\n",
    "\n",
    "print(\"Predictions\", end=\"\")\n",
    "print(model.predict(x))\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_nonlinear_model_clas(x, y_xor, model, phi=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "Execute the cell several times.\n",
    "* How does the model perform?\n",
    "* Is it able to separate the dataset eventually?\n",
    "\n",
    "Change the architecture to include $10$ hidden units (`hidden_layer_sizes=(10,)`).\n",
    "* Is the model better now?\n",
    "* Is it more consistent?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains an MLP for classification with the default parameters, a single hidden layer of $100$ units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(max_iter=1000)\n",
    "model.fit(x_moon, y_moon)\n",
    "\n",
    "plot_nonlinear_model_clas(x_moon, y_moon, model, phi=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the model perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual approach is to hyper-parametrize the model, for example using cross-validation, as shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(MLPClassifier(),\n",
    "                  param_grid={\"hidden_layer_sizes\": [(10,), (100,), (200,),\n",
    "                                                     (10, 10), (100, 100), (200, 200)]})\n",
    "\n",
    "gs.fit(x_moon, y_moon)\n",
    "print(gs.best_params_)\n",
    "\n",
    "plot_nonlinear_model_clas(x_moon, y_moon, gs.best_estimator_, phi=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the resultant model better than the one before?\n",
    "* Which one seems to be the optimal architecture?\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
