{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"title\">Support Vector Machines</div>\n",
    "<div class=\"subtitle\">Métodos Avanzados en Aprendizaje Automático</div>\n",
    "<div class=\"author\">Carlos María Alaíz Gudín - Universidad Autónoma de Madrid</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the configuration of Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<head><link rel=\"stylesheet\" href=\"style.css\"></head>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<head><link rel=\"stylesheet\" href=\"style.css\"></head>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the packages to be used (all of them quite standard except for `Utils`, which is provided with the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.svm import SVC, SVR, OneClassSVM\n",
    "\n",
    "from Utils import plot_dataset_clas, plot_dataset\n",
    "from Utils import plot_svc, plot_all_linear_separators, plot_svr\n",
    "\n",
    "matplotlib.rc('figure', figsize=(15, 5))\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC: Multiple Hyperplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is simply a 2-dimensional binary classification dataset, with two clearly separable classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 100\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x, y = make_blobs(n_samples=n_pat, n_features=2,\n",
    "                  cluster_std=2.0, random_state=seed)\n",
    "y[y != 1] = -1\n",
    "plot_dataset_clas(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Perfect Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different hyperplanes (in this case, simply straight lines) that can be used to separate both classes.\n",
    "This code represents some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_linear_separators(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Which linear model (straight line) seems the best option to separate the classes? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell highlight the model that it is somehow \"in the middle\" of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_linear_separators(x, y, plot_best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does this model coincides with your intuition of the best one?\n",
    "* Why do you think those three samples are circled?\n",
    "* Does the model in the middle depends on any other sample, apart from these three?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard-Margin SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model over the Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below represents a Hard-Margin SVC trained over the previous dataset.\n",
    "All the samples satisfy the condition $y_i (\\mathbf{w}^\\intercal \\mathbf{x}_i + b) \\ge 1$ (the borders of such conditions are the supporting hyperplanes, represented with dashed lines), and the margin is maximum.\n",
    "The solid line represents the separating hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=1e5, kernel='linear')\n",
    "model.fit(x, y)\n",
    "\n",
    "plot_svc(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Why do you think those three samples are circled?\n",
    "* Does the margin depends on any other sample, apart from these three?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard-Margin SVC: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell accesses the support vectors inside the model.\n",
    "According to the theory, these points should be over the supporting hyperplane (i.e., $(\\mathbf{w}^\\intercal \\mathbf{x}_i + b) = y_i = \\pm1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index of support vectors:\", model.support_)\n",
    "print(\"Support vectors:\\n\", model.support_vectors_)\n",
    "print(\"Prediction over the support vectors:\", model.decision_function(model.support_vectors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does these support vectors coincide with the circled points of the previous plot?\n",
    "* Check the prediction of the model over the support vectors. Does this match the theory?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the Primal Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the primal hyperplane, the following equation can be used:\n",
    "$$ \\mathbf{w} = \\sum_{i = 1}^N y_i \\alpha_i \\mathbf{x}_i . $$\n",
    "Moreover, since for a support vector $(\\mathbf{w}^\\intercal \\mathbf{x}_i + b) = y_i$, then the bias $b$ can be recovered from any support vector as:\n",
    "$$ b = y_i - \\mathbf{w} \\mathbf{x}_i .$$\n",
    "\n",
    "The following cell recovers the primal linear model $\\{b, \\mathbf{w}\\}$ from the dual variables (it should be noted that the coefficients `model.dual_coef_` correspond already to $y_i \\alpha_i$).\n",
    "These primal variables can also be accessed directly through `model.coef_` and `model.intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.dual_coef_[0] @ model.support_vectors_\n",
    "print(\"Primal hyperplane (computed):    \", w)\n",
    "print(\"Primal hyperplane (from sklearn):\", model.coef_[0])\n",
    "\n",
    "b = y[model.support_] - x[model.support_, :] @ w\n",
    "print(\"\")\n",
    "print(\"Primal bias (computed from every support vector):\", b)\n",
    "print(\"Primal bias (from sklearn):                      \", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does this code match the theory?\n",
    "* Is the primal model being recovered correctly?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model over the Additional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVMs are sparse models, based only on certain samples (the support vectors), which are located intuitively near the other class.\n",
    "The following cell adds six additional points to the original dataset, but far away from the border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ext = np.vstack((x, np.array([[-12, 0], [12, 0], [-12, -5], [12, -5], [-12, 5], [12, 5]])))\n",
    "y_ext = np.append(y, [1, -1, 1, -1, 1, -1])\n",
    "\n",
    "plot_dataset_clas(x_ext, y_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Do you think that these points will affect the model? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cells trains another SVM over the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=1e3, kernel='linear')\n",
    "model.fit(x_ext, y_ext)\n",
    "\n",
    "plot_svc(x_ext, y_ext, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the model different from the previous one? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft-Margin SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following dataset an overlapping between classes is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 50\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x, y =  make_blobs(n_samples=n_pat, n_features=2, cluster_std=3.0, random_state=seed)\n",
    "y[y != 1] = -1\n",
    "plot_dataset_clas(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Are both classes still linearly separable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Soft-Margin SVC is trained over this dataset.\n",
    "The slack variables (distance to the corresponding supporting hyperplane) are also represented as dotted lines.\n",
    "\n",
    "The hyper-parameter $C$ is controlled by the variable `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=1e0, kernel='linear')\n",
    "model.fit(x, y)\n",
    "\n",
    "plot_svc(x, y, model, plot_slack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Are all the points correctly classified?\n",
    "* The slack variables can be larger than $0$ even if the point is still correctly classified. Can you distinguish this on the plot?\n",
    "* According to the theory, the support vectors are either over the corresponding supporting hyperplane or on the wrong side of it. Can this be seen in the plot?\n",
    "* Try different values of $C$ like $10^{-2}$ and $10^{2}$.\n",
    "    - How does the margin change?\n",
    "    - How does the training error change?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates a regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 20\n",
    "noise = 5e-1\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x = np.linspace(-3, 3, n_pat)\n",
    "y = x + noise * np.random.randn(n_pat)\n",
    "\n",
    "plot_dataset(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the problem (approximately) linear?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell trains an SVR over the previous dataset, and depicts it.\n",
    "The support vectors (those outside the $\\epsilon$-insensitive tube, or in the border) are the circled samples. The slack variables (distance to the $\\epsilon$-insensitive tube) are represented as dotted lines.\n",
    "\n",
    "The hyper-parameter $\\epsilon$ is controlled by the variable `epsilon`, and $C$ by the variable `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(epsilon=1e0, C=1e0, kernel='linear')\n",
    "model.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "plot_svr(x, y, model, plot_slack=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Try to change the value of $C$ to $10^{-2}$ and $10^{2}$. Is it possible to induce over-fitting? Why? Notice that in this case the model is still linear.\n",
    "* What happens when you decrease the value of $\\epsilon$ to $10^{-2}$? And when you increase it to $10^{1}$? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell accesses the support vectors inside the model.\n",
    "According to the theory, these points should be over or outside the $\\epsilon$-insensitive tube (i.e., $| y_i - (\\mathbf{w}^\\intercal \\mathbf{x}_i + b) | \\ge \\epsilon$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index of support vectors:\", model.support_)\n",
    "print(\"Support vectors:\\n\", model.support_vectors_)\n",
    "print(\"Prediction over the support vectors:\", model.predict(model.support_vectors_))\n",
    "print(\"Residual of the prediction over the support vectors:\", y[model.support_] - model.predict(model.support_vectors_))\n",
    "print(\"Value of epsilon:\", model.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does these support vectors coincide with the circled points of the previous plot?\n",
    "* Check the prediction of the model over the support vectors. Does this match the theory?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the Primal Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the primal hyperplane, the following equation can be used:\n",
    "$$ \\mathbf{w} = \\sum_{i = 1}^N (\\alpha^*_i - \\alpha_i) \\mathbf{x}_i . $$\n",
    "In this case, it is slightly more involved to compute the value of $b$, so it will be omitted.\n",
    "\n",
    "The following cell recovers the primal hyperplane $\\mathbf{w}$ from the dual variables (it should be noted that the coefficients `model.dual_coef_` correspond already to $\\alpha^*_i - \\alpha_i$).\n",
    "This primal variable can also be accessed directly through `model.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.dual_coef_[0] @ model.support_vectors_\n",
    "print(\"Primal hyperplane (computed):    \", w)\n",
    "print(\"Primal hyperplane (from sklearn):\", model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does this code match the theory?\n",
    "* Is the primal model being recovered correctly?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates an unsupervised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 100\n",
    "n_out = 5\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x = 1e-1 * np.random.randn(n_pat - n_out, 2) + 1\n",
    "x = np.vstack((x, 1e-1 * np.random.randn(n_out, 2)))\n",
    "y = np.ones(n_pat)\n",
    "plot_dataset_clas(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is it possible to detect the outliers by hand?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell trains a OC-SVM over the previous dataset, and depicts it.\n",
    "The support vectors are the circled samples. The slack variables are represented as dotted lines.\n",
    "\n",
    "The hyper-parameter $\\nu$ is controlled by the variable `nu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneClassSVM(kernel='linear', nu=0.5)\n",
    "\n",
    "model.fit(x)\n",
    "\n",
    "plot_svc(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Try to change the value of $\\nu$ to $0.01$, $0.1$ and $0.9$. What is the effect? Notice that in this case the model is still linear.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC-SVM: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell accesses the support vectors inside the model.\n",
    "According to the theory, these points should satisfy $\\mathbf{w}^\\intercal \\mathbf{x}_i - \\rho \\le 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index of support vectors:\", model.support_)\n",
    "print(\"Support vectors:\\n\", model.support_vectors_)\n",
    "print(\"Prediction over the support vectors:\", model.decision_function(model.support_vectors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does these support vectors coincide with the circled points of the previous plot?\n",
    "* Check the prediction of the model over the support vectors. Does this match the theory?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the Primal Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the primal hyperplane, the following equation can be used:\n",
    "$$ \\mathbf{w} = \\sum_{i = 1}^N \\alpha_i \\mathbf{x}_i . $$\n",
    "\n",
    "The following cell recovers the primal hyperplane $\\mathbf{w}$ from the dual variables.\n",
    "This primal variable can also be accessed directly through `model.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.dual_coef_[0] @ model.support_vectors_\n",
    "print(\"Primal hyperplane (computed):    \", w)\n",
    "print(\"Primal hyperplane (from sklearn):\", model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does this code match the theory?\n",
    "* Is the primal model being recovered correctly?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates a 2-dimensional classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 100\n",
    "noise = 0.1\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x, y = make_moons(n_samples=n_pat, noise=noise, random_state=seed)\n",
    "y[y == 0] = -1\n",
    "plot_dataset_clas(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the dataset linearly separable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a linear SVM over the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(x, y)\n",
    "\n",
    "plot_svc(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the previous model perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell trains a non-linear SVM with RBF kernel, where the parameter of the kernel $\\gamma$ is given by the variable `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', gamma='scale', C=1e0)\n",
    "model.fit(x, y)\n",
    "\n",
    "plot_svc(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does it perform?\n",
    "* Try modifying the value of $C$ to $10^{-2}$ and $10^2$. Is the model under-fitting or over-fitting the data?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates a regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 50\n",
    "noise = 5e-1\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x = np.linspace(-3, 3, n_pat)\n",
    "y = x**2 + 5 * np.sin(2 * x) + noise * np.random.randn(n_pat)\n",
    "\n",
    "plot_dataset(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the problem (approximately) linear?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a linear SVM over the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='linear', epsilon=1e0, C=1e1)\n",
    "model.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "plot_svr(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the previous model perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell trains a non-linear SVM with RBF kernel, where the parameter of the kernel $\\gamma$ is given by the variable `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='rbf', epsilon=1e0, C=1e1, gamma='scale')\n",
    "model.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "plot_svr(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the previous model perform?\n",
    "* Try modifying the value of $C$ to $10^{-2}$ and $10^2$. Is the model under-fitting or over-fitting the data?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear OC-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates an unsupervised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 500\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x, y =  make_circles(n_samples=n_pat, noise=1e-1, random_state=seed)\n",
    "x = x[y == 1] + 10\n",
    "y = y[y == 1]\n",
    "plot_dataset_clas(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the dataset linear?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a linear OC-SVM over the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneClassSVM(kernel='linear', nu=0.5)\n",
    "\n",
    "model.fit(x)\n",
    "\n",
    "plot_svc(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the previous model perform?\n",
    "* Does it model correctly the dataset?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell trains a non-linear OC-SVM with RBF kernel, where the parameter of the kernel $\\gamma$ is given by the variable `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneClassSVM(kernel='rbf', gamma='scale',nu=0.5)\n",
    "\n",
    "model.fit(x)\n",
    "\n",
    "plot_svc(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the previous model perform?\n",
    "* Try modifying the value of $\\nu$ to $0.01$, $0.1$ and $0.9$. Is the model under-fitting or over-fitting the data distribution?    \n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
